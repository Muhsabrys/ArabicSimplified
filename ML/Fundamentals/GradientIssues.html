<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Vanishing & Exploding Gradients Problem</title>

    <!-- Tailwind & Fonts -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f5f7fa;
            color: #1d2a39;
        }

        .card {
            background-color: #ffffff;
            border-radius: 0.75rem;
            padding: 1.75rem;
            box-shadow: 0 6px 16px rgba(0, 0, 0, 0.06);
        }

        .chart-container {
            position: relative;
            width: 100%;
            height: 320px;
        }

        @media (min-width: 768px) {
            .chart-container { height: 380px; }
        }

        .flow-step .step-num {
            width: 2.7rem;
            height: 2.7rem;
            display: flex;
            align-items: center;
            justify-content: center;
            background-color: #0A2463;
            color: #fff;
            border-radius: 9999px;
            font-weight: 700;
            font-size: 1.1rem;
        }

        .flow-arrow {
            font-size: 2rem;
            color: #3E92CC;
            font-weight: 700;
        }
    </style>
</head>

<body>

    <div class="max-w-7xl mx-auto p-6 md:p-10">

        <!-- Header -->
        <header class="text-center mb-12">
            <h1 class="text-4xl md:text-5xl font-extrabold text-[#0A2463] mb-4">
                The Vanishing & Exploding Gradients Problem
            </h1>
            <p class="text-lg md:text-xl text-gray-700 max-w-3xl mx-auto">
                A clear, intuitive guide to one of the biggest obstacles in deep learning—and how modern AI avoids it.
            </p>
        </header>

        <!-- Section: Ideal Learning -->
        <section class="mb-14">
            <h2 class="text-3xl font-bold text-[#0A2463] text-center mb-8">
                How Backpropagation Works (The Ideal Case)
            </h2>

            <p class="text-center text-gray-700 max-w-2xl mx-auto mb-10">
                Deep neural networks learn using <strong>backpropagation</strong>, a method where the final error is sent backward through the network as “correction signals” (gradients).  
                Each layer adjusts itself slightly based on this signal.
            </p>

            <div class="card">
                <div class="grid grid-cols-1 md:grid-cols-5 gap-6 items-center">

                    <div class="flow-step text-center">
                        <div class="step-num mb-3">1</div>
                        <h3 class="font-semibold text-lg mb-1">Set the Goal</h3>
                        <p class="text-sm text-gray-600">e.g., “Identify this image as a cat.”</p>
                    </div>

                    <div class="flow-arrow hidden md:flex justify-center">➡️</div>
                    <div class="flow-arrow md:hidden text-center">⬇️</div>

                    <div class="flow-step text-center">
                        <div class="step-num mb-3">2</div>
                        <h3 class="font-semibold text-lg mb-1">Make a Prediction</h3>
                        <p class="text-sm text-gray-600">Network guesses: “Dog”.</p>
                    </div>

                    <div class="flow-arrow hidden md:flex justify-center">➡️</div>
                    <div class="flow-arrow md:hidden text-center">⬇️</div>

                    <div class="flow-step text-center">
                        <div class="step-num mb-3">3</div>
                        <h3 class="font-semibold text-lg mb-1">Compute Error</h3>
                        <p class="text-sm text-gray-600">Compare guess vs. correct label.</p>
                    </div>

                    <div class="flow-arrow hidden md:flex justify-center">➡️</div>
                    <div class="flow-arrow md:hidden text-center">⬇️</div>

                    <div class="flow-step text-center">
                        <div class="step-num mb-3">4</div>
                        <h3 class="font-semibold text-lg mb-1">Backpropagate</h3>
                        <p class="text-sm text-gray-600">Send correction signals backward.</p>
                    </div>

                    <div class="flow-arrow hidden md:flex justify-center">➡️</div>
                    <div class="flow-arrow md:hidden text-center">⬇️</div>

                    <div class="flow-step text-center">
                        <div class="step-num mb-3">5</div>
                        <h3 class="font-semibold text-lg mb-1">Update Layers</h3>
                        <p class="text-sm text-gray-600">Each layer adjusts slightly.</p>
                    </div>

                </div>
            </div>
        </section>


        <!-- Section: The Problem -->
        <section class="mb-14">
            <h2 class="text-3xl font-bold text-center text-[#0A2463] mb-8">The Problem: Weak or Unstable Gradients</h2>

            <p class="text-center text-gray-700 max-w-2xl mx-auto mb-10">
                In deep networks, gradients must travel through many layers.  
                Repeated multiplication can make them <strong>shrink to zero</strong> or <strong>blow up uncontrollably</strong>.
            </p>

            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">

                <!-- Vanishing -->
                <div class="card border-t-4 border-red-500">
                    <h3 class="text-2xl font-bold text-red-600 mb-4">Vanishing Gradients ⬇️</h3>
                    <p class="text-gray-700 mb-4">
                        Gradients become extremely small after being multiplied many times.  
                        Early layers receive almost no learning signal.
                    </p>

                    <div class="bg-red-50 p-4 rounded-lg">
                        <p class="text-sm font-semibold text-red-600">Final Layer Gradient:</p>
                        <p class="text-xl font-bold text-red-600">0.5</p>

                        <p class="text-sm font-semibold text-red-600 mt-3">First Layer Gradient:</p>
                        <p class="text-xl font-bold text-red-600">0.0000001</p>
                    </div>

                    <p class="mt-4 font-semibold text-gray-700">
                        ➤ Early layers stop learning → model fails.
                    </p>
                </div>

                <!-- Exploding -->
                <div class="card border-t-4 border-red-500">
                    <h3 class="text-2xl font-bold text-red-600 mb-4">Exploding Gradients ⬆️</h3>
                    <p class="text-gray-700 mb-4">
                        Gradients grow larger as they propagate, becoming unstable.
                    </p>

                    <div class="bg-red-50 p-4 rounded-lg">
                        <p class="text-sm font-semibold text-red-600">Final Layer Gradient:</p>
                        <p class="text-xl font-bold text-red-600">1.5</p>

                        <p class="text-sm font-semibold text-red-600 mt-3">First Layer Gradient:</p>
                        <p class="text-xl font-bold text-red-600">1,500,000</p>
                    </div>

                    <p class="mt-4 font-semibold text-gray-700">
                        ➤ Learning becomes chaotic → training collapses.
                    </p>
                </div>

            </div>
        </section>

        <!-- Section: Sigmoid Culprit -->
        <section class="mb-14">
            <h2 class="text-3xl font-bold text-center text-[#0A2463] mb-8">
                Why Sigmoid Made Things Worse
            </h2>

            <p class="text-center text-gray-700 max-w-2xl mx-auto mb-10">
                The sigmoid activation function saturates on both ends.  
                In these flat regions, the gradient becomes almost zero → gradients vanish.
            </p>

            <div class="card">
                <h3 class="text-xl font-bold text-[#3E92CC] text-center mb-4">
                    Sigmoid Output and Its Gradient
                </h3>

                <div class="chart-container">
                    <canvas id="sigmoidChart"></canvas>
                </div>

                <p class="text-center text-sm text-gray-600 mt-4">
                    The gradient (dark line) is strong only in the middle.  
                    Everywhere else → almost zero → no learning signal.
                </p>
            </div>
        </section>

        <!-- Visualization of effects -->
        <section class="mb-14">
            <h2 class="text-3xl font-bold text-center text-[#0A2463] mb-8">
                What Vanishing vs. Exploding Actually Looks Like
            </h2>

            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                <!-- Vanishing -->
                <div class="card">
                    <h3 class="text-xl font-bold text-red-600 text-center mb-4">
                        Vanishing Gradient Effect
                    </h3>
                    <div class="chart-container">
                        <canvas id="vanishingChart"></canvas>
                    </div>
                    <p class="text-center text-sm text-gray-600 mt-4">
                        Gradients shrink rapidly—early layers learn almost nothing.
                    </p>
                </div>

                <!-- Exploding -->
                <div class="card">
                    <h3 class="text-xl font-bold text-red-600 text-center mb-4">
                        Exploding Gradient Effect
                    </h3>
                    <div class="chart-container">
                        <canvas id="explodingChart"></canvas>
                    </div>
                    <p class="text-center text-sm text-gray-600 mt-4">
                        Gradients blow up—training becomes unstable.
                    </p>
                </div>
            </div>
        </section>
        
        
        <section class="mb-14">
    <h2 class="text-3xl font-bold text-center text-[#0A2463] mb-8">
        Solution 3: Glorot (Xavier) Initialization
    </h2>

    <p class="text-center text-gray-700 max-w-3xl mx-auto mb-10">
        Glorot Initialization — also known as <strong>Xavier Initialization</strong> — is one of the foundational techniques 
        that prevents vanishing and exploding gradients <strong>before training even begins</strong>.  
        It works by choosing initial weights that keep the signal size stable as it flows through layers.
    </p>

    <div class="card border-t-4 border-blue-600">
        <h3 class="text-xl font-bold text-blue-700 text-center mb-4">Why It's Needed</h3>

        <p class="text-gray-700 mb-6">
            In deep networks, values are repeatedly multiplied:
        </p>

        <ul class="list-disc pl-6 text-gray-700 space-y-2 mb-6">
            <li>If weights are <strong>too small</strong>, signals shrink → <span class="text-red-600 font-semibold">vanishing gradients</span>.</li>
            <li>If weights are <strong>too large</strong>, signals explode → <span class="text-red-600 font-semibold">exploding gradients</span>.</li>
        </ul>

        <p class="text-gray-700 mb-6">
            Glorot Initialization solves this by ensuring that:
        </p>

        <p class="bg-blue-50 p-4 rounded-lg border border-blue-200 text-gray-800 text-center font-semibold">
            <strong>The variance of a layer’s outputs ≈ the variance of its inputs</strong><br>
            (for both the forward pass and backward pass).
        </p>
    </div>

    <div class="card mt-8">
        <h3 class="text-xl font-bold text-blue-700 text-center mb-4">How It Works</h3>

        <p class="text-gray-700 mb-6">
            The initialization depends on the number of incoming and outgoing connections:
        </p>

        <div class="grid grid-cols-1 md:grid-cols-2 gap-8 mb-8">
            <div>
                <h4 class="font-semibold text-gray-800 mb-2">Layer Statistics</h4>
                <ul class="list-disc pl-6 text-gray-700 space-y-2">
                    <li><strong>fan<sub>in</sub></strong>: number of neurons feeding into the layer</li>
                    <li><strong>fan<sub>out</sub></strong>: number of neurons the layer feeds into</li>
                </ul>
            </div>

            <div>
                <h4 class="font-semibold text-gray-800 mb-2">Average Fan</h4>
                <p class="text-gray-700">
                    The scaling factor is computed as:
                </p>
                <p class="text-center font-mono text-lg bg-gray-50 p-3 rounded-lg border mt-2">
                    fan<sub>avg</sub> = (fan<sub>in</sub> + fan<sub>out</sub>) / 2
                </p>
            </div>
        </div>

        <p class="text-gray-700 mb-6">
            The weights are then sampled from either a normal distribution or a uniform distribution, each scaled appropriately:
        </p>

        <div class="grid grid-cols-1 md:grid-cols-2 gap-8">

            <!-- Normal Distribution -->
            <div class="bg-blue-50 p-5 rounded-lg border border-blue-200">
                <h4 class="text-lg font-bold text-blue-700 mb-3 text-center">
                    Normal (Gaussian) Initialization
                </h4>
                <p class="text-gray-700 text-center mb-3">Weights are drawn from:</p>
                <p class="font-mono text-center bg-white py-2 px-3 rounded border text-gray-800">
                    W ~ N(0, 1 / fan<sub>avg</sub>)
                </p>
                <p class="text-center text-sm text-gray-600 mt-3">
                    Variance is inversely proportional to the average number of connections.
                </p>
            </div>

            <!-- Uniform Distribution -->
            <div class="bg-blue-50 p-5 rounded-lg border border-blue-200">
                <h4 class="text-lg font-bold text-blue-700 mb-3 text-center">
                    Uniform Initialization
                </h4>
                <p class="text-gray-700 text-center mb-3">Weights are sampled from:</p>
                <p class="font-mono text-center bg-white py-2 px-3 rounded border text-gray-800">
                    W ~ U(−r, +r)
                </p>
                <p class="font-mono text-center bg-white py-2 px-3 rounded border text-gray-800 mt-2">
                    r = √(6 / (fan<sub>in</sub> + fan<sub>out</sub>))
                </p>
                <p class="text-center text-sm text-gray-600 mt-3">
                    Ensures balanced variance for both forward and backward paths.
                </p>
            </div>

        </div>

        <p class="text-gray-700 mt-8 text-center max-w-2xl mx-auto">
            This careful scaling ensures that information flows cleanly through deep networks  
            — neither fading away nor blowing up — making Glorot Initialization a foundational technique  
            for training modern neural architectures.
        </p>
    </div>
</section>


        <!-- Solutions -->
        <section class="mb-14">
            <h2 class="text-3xl font-bold text-center text-[#0A2463] mb-8">
                Modern Solutions that Fixed the Problem
            </h2>

            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">

                <!-- ReLU -->
                <div class="card border-t-4 border-green-600">
                    <h3 class="text-xl font-bold text-green-700 text-center mb-4">
                        Solution 1: ReLU Activation
                    </h3>

                    <div class="chart-container">
                        <canvas id="reluChart"></canvas>
                    </div>

                    <p class="text-center text-sm text-gray-700 mt-4">
                        ReLU never saturates on the positive side.  
                        Its gradient = <strong>1</strong> → gradients do not vanish.
                    </p>
                </div>

                <!-- Gradient Clipping -->
                <div class="card border-t-4 border-green-600">
                    <h3 class="text-xl font-bold text-green-700 text-center mb-4">
                        Solution 2: Gradient Clipping
                    </h3>

                    <p class="text-gray-700 mb-4">
                        This protects training from exploding gradients.  
                        If a gradient exceeds a maximum threshold, we “clip” it down.
                    </p>

                    <div class="bg-green-50 p-4 rounded-lg">
                        <h4 class="font-semibold text-green-700 mb-3">Example</h4>
                        <div class="flex items-center justify-around">

                            <div class="text-center">
                                <p class="text-sm font-semibold text-red-600">Original Gradient</p>
                                <div class="w-16 h-28 bg-red-500 rounded mx-auto mt-2"></div>
                                <p class="font-bold text-red-600 mt-1 text-lg">15.7</p>
                            </div>

                            <div class="text-3xl mx-3 text-blue-500 font-bold">➡️</div>

                            <div class="text-center">
                                <p class="text-sm font-semibold text-green-700">Clipped Gradient</p>
                                <div class="w-16 h-14 bg-green-500 rounded mx-auto mt-2"></div>
                                <p class="font-bold text-green-700 text-lg mt-1">1.0</p>
                                <p class="text-xs text-gray-600">(Max threshold)</p>
                            </div>

                        </div>
                    </div>
                </div>

            </div>
        </section>


        <!-- Footer -->
        <footer class="text-center mt-12 pt-6 border-t border-gray-300">
            <p class="text-gray-600 text-sm">
                Charts rendered with Chart.js · Styled with TailwindCSS · Designed for clarity and educational use.
            </p>
        </footer>

    </div>


    <!-- JS (Charts) -->
    <script>
        (function () {
            const tooltipFix = {
                plugins: {
                    tooltip: {
                        callbacks: {
                            title: function (items) {
                                const label = items[0].label;
                                return Array.isArray(label) ? label.join(' ') : label;
                            }
                        }
                    }
                }
            };

            // Sigmoid Function
            const sigmoidData = () => {
                const labels = [];
                const y = [];
                const dydx = [];
                for (let x = -8; x <= 8; x += 0.5) {
                    labels.push(x);
                    const s = 1 / (1 + Math.exp(-x));
                    y.push(s);
                    dydx.push(s * (1 - s));
                }
                return { labels, y, dydx };
            };

            const { labels: sLbl, y: sVal, dydx: sGrad } = sigmoidData();
            new Chart(document.getElementById('sigmoidChart'), {
                type: 'line',
                data: {
                    labels: sLbl,
                    datasets: [
                        {
                            label: "Sigmoid Output",
                            data: sVal,
                            borderColor: "#3E92CC",
                            backgroundColor: "rgba(62, 146, 204, 0.15)",
                            borderWidth: 3,
                            pointRadius: 0
                        },
                        {
                            label: "Gradient",
                            data: sGrad,
                            borderColor: "#0A2463",
                            borderDash: [5, 5],
                            backgroundColor: "rgba(10, 36, 99, 0.15)",
                            borderWidth: 3,
                            pointRadius: 0
                        }
                    ]
                },
                options: { responsive: true, maintainAspectRatio: false, ...tooltipFix }
            });

            // Vanishing
            const vanLbl = [];
            const vanVal = [];
            let g = 1.0;
            for (let i = 20; i >= 1; i--) {
                vanLbl.push("Layer " + i);
                vanVal.push(g);
                g *= 0.65;
            }

            new Chart(document.getElementById('vanishingChart'), {
                type: "line",
                data: {
                    labels: vanLbl,
                    datasets: [{
                        label: "Gradient Magnitude",
                        data: vanVal,
                        borderColor: "#FF5733",
                        backgroundColor: "rgba(255, 87, 51, 0.2)",
                        fill: true,
                        tension: 0.2
                    }]
                },
                options: { responsive: true, maintainAspectRatio: false, ...tooltipFix }
            });

            // Exploding
            const expLbl = [];
            const expVal = [];
            g = 1.0;
            for (let i = 20; i >= 1; i--) {
                expLbl.push("Layer " + i);
                g *= 1.3;
                if (g > 50) g = 50;
                expVal.push(g);
            }

            new Chart(document.getElementById('explodingChart'), {
                type: "line",
                data: {
                    labels: expLbl,
                    datasets: [{
                        label: "Gradient Magnitude",
                        data: expVal,
                        borderColor: "#FF5733",
                        backgroundColor: "rgba(255, 87, 51, 0.2)",
                        fill: true,
                        tension: 0.2
                    }]
                },
                options: { responsive: true, maintainAspectRatio: false, ...tooltipFix }
            });

            // ReLU
            const relLbl = [];
            const rel = [];
            const relG = [];
            for (let x = -8; x <= 8; x++) {
                relLbl.push(x);
                rel.push(Math.max(0, x));
                relG.push(x > 0 ? 1 : 0);
            }

            new Chart(document.getElementById('reluChart'), {
                type: "line",
                data: {
                    labels: relLbl,
                    datasets: [
                        {
                            label: "ReLU Output",
                            data: rel,
                            borderColor: "#3E92CC",
                            backgroundColor: "rgba(62, 146, 204, 0.15)",
                            borderWidth: 3,
                            stepped: true
                        },
                        {
                            label: "Gradient",
                            data: relG,
                            borderColor: "#28A745",
                            backgroundColor: "rgba(40, 167, 69, 0.15)",
                            borderWidth: 3,
                            stepped: true
                        }
                    ]
                },
                options: { responsive: true, maintainAspectRatio: false, ...tooltipFix }
            });

        })();
    </script>


    <!-- Global AI Assistant -->
    <script src="../ai-assistant.js"></script>

</body>
</html>
